{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a2ccdd5-c372-489c-8aee-f7bb0fe8340a",
   "metadata": {},
   "source": [
    "# Flipkart single page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f70cf891-153a-4f2b-9df8-7881828648fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved the page\n",
      "<Response [200]>\n",
      "                                         Product_Name Rating    Price\n",
      "0   OPPO K12x 5G with 45W SUPERVOOC Charger In-The...    4.5  ₹12,999\n",
      "1               CMF by Nothing Phone 1 (Blue, 128 GB)    4.4  ₹16,499\n",
      "2                  REDMI 13C (Stardust Black, 128 GB)    4.3   ₹7,199\n",
      "3   Motorola Edge 50 Fusion (Marshmallow Blue, 256...    4.5  ₹23,999\n",
      "4             vivo V30 Pro 5G (Classic Black, 256 GB)    4.5  ₹38,999\n",
      "5                 REDMI 13C (Starfrost White, 128 GB)    4.3   ₹7,199\n",
      "6                      Apple iPhone 15 (Blue, 128 GB)    4.6  ₹57,999\n",
      "7            vivo T3 Lite 5G (Majestic Black, 128 GB)    4.4  ₹11,499\n",
      "8             vivo T3 Lite 5G (Vibrant Green, 128 GB)    4.4  ₹11,499\n",
      "9            vivo T3 Lite 5G (Majestic Black, 128 GB)    4.4  ₹10,499\n",
      "10            vivo T3 Lite 5G (Vibrant Green, 128 GB)    4.4  ₹10,499\n",
      "11           SAMSUNG Galaxy A14 5G (Dark Red, 128 GB)    4.2  ₹10,999\n",
      "12                  realme 12x 5G (Coral Red, 128 GB)    4.5  ₹13,499\n",
      "13          SAMSUNG Galaxy F05 (Twilight Blue, 64 GB)    4.2   ₹6,499\n",
      "14           Motorola g45 5G (Brilliant Blue, 128 GB)    4.4  ₹11,999\n",
      "15                vivo T3x 5G (Crimson Bliss, 128 GB)    4.5  ₹14,499\n",
      "16                vivo T3x 5G (Crimson Bliss, 128 GB)    4.5  ₹12,999\n",
      "17              vivo T3x 5G (Celestial Green, 128 GB)    4.5  ₹14,499\n",
      "18          Motorola g45 5G (Brilliant Green, 128 GB)    4.4  ₹11,999\n",
      "19              vivo T3x 5G (Celestial Green, 128 GB)    4.5  ₹12,999\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL = \"https://www.flipkart.com/search?q=mobiles&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    'Accept-Language': 'en-US, en;q=0.5'\n",
    "}\n",
    "\n",
    "response = requests.get(URL, headers=HEADERS)  #headers added to avoid server errors\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully retrieved the page\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page, status code: {response.status_code}\")\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "print(response)\n",
    "\n",
    "product_names = []\n",
    "rating = []\n",
    "price = []\n",
    "\n",
    "\n",
    "columns = {\"Product_Name\":[],\"Rating\":[],\"Price\":[]}\n",
    "\n",
    "\n",
    "\n",
    "for product in soup.find_all(\"div\", class_=\"KzDlHZ\"):\n",
    "    product_names.append(product.get_text())\n",
    "    columns[\"Product_Name\"].append(product.get_text())\n",
    "    if len(product_names)==20:\n",
    "         break\n",
    "\n",
    "\n",
    "for ratingg in soup.find_all(\"div\", class_=\"XQDdHH\"):\n",
    "    rating.append(ratingg.get_text())\n",
    "    columns[\"Rating\"].append(ratingg.get_text())\n",
    "    if len(rating)==20:\n",
    "        break\n",
    "\n",
    "for pricee in soup.find_all(\"div\", class_=\"Nx9bqj _4b5DiR\"):\n",
    "    price.append(pricee.get_text())\n",
    "    columns[\"Price\"].append(pricee.get_text())\n",
    "    if len(price)==20:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "scraped_flip = pd.DataFrame.from_dict(columns)\n",
    "print(scraped_flip)\n",
    "scraped_flip.to_csv(\"scraped_flip1.csv\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf216c-5705-4cb7-9a0a-744ac7005dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af5083a8-a5ca-492d-87d9-68270d16362e",
   "metadata": {},
   "source": [
    "# FLipkart Multi Page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f56b383b-91c4-400c-a854-f7e991461b48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved the page 1\n",
      "Successfully retrieved the page 2\n",
      "Successfully retrieved the page 3\n",
      "Successfully retrieved the page 4\n",
      "Successfully retrieved the page 5\n",
      "Successfully retrieved the page 6\n",
      "Successfully retrieved the page 7\n",
      "Successfully retrieved the page 8\n",
      "Successfully retrieved the page 9\n",
      "Successfully retrieved the page 10\n",
      "0\n",
      "200\n",
      "200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ratings))\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(prices))\n\u001b[1;32m---> 64\u001b[0m scraped_flip \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(columns)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(scraped_flip)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:1917\u001b[0m, in \u001b[0;36mDataFrame.from_dict\u001b[1;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[0;32m   1911\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1912\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for orient parameter. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1913\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morient\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1914\u001b[0m     )\n\u001b[0;32m   1916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(data, index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1918\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1919\u001b[0m     realdata \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "BASE_URL = \"https://www.flipkart.com/search?q=mobiles&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    'Accept-Language': 'en-US, en;q=0.5'\n",
    "}\n",
    "\n",
    "product_names = []\n",
    "ratings = []\n",
    "prices = []\n",
    "\n",
    "NUM_PAGES = 10\n",
    "\n",
    "\n",
    "for page in range(1, NUM_PAGES + 1):\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    URL = BASE_URL + str(page)\n",
    "    response = requests.get(URL, headers=HEADERS)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Successfully retrieved the page {page}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page {page}, status code: {response.status_code}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    for product in soup.find_all(\"div\", class_=\"KzDlHZ\"):\n",
    "        product_names.append(product.get_text())\n",
    "        \n",
    "\n",
    "    for rating in soup.find_all(\"div\", class_=\"XQDdHH\"):\n",
    "        ratings.append(rating.get_text())\n",
    "        \n",
    "    for price in soup.find_all(\"div\", class_=\"Nx9bqj _4b5DiR\"):\n",
    "        prices.append(price.get_text())\n",
    "        \n",
    "\n",
    "\n",
    "def set_100(max_num):\n",
    "    if len(product_names)>max_num:\n",
    "        product_names[:]=[]\n",
    "    if len(ratings)>max_num:\n",
    "        ratings[max_num:]=[]\n",
    "    if len(prices)>max_num:\n",
    "        prices[max_num:]=[]\n",
    "    \n",
    "set_100(200)\n",
    "\n",
    "\n",
    "columns = {\"Product_Name\": product_names, \"Rating\": ratings, \"Price\": prices}\n",
    "\n",
    "print(len(product_names))\n",
    "print(len(ratings))\n",
    "print(len(prices))\n",
    "\n",
    "scraped_flip = pd.DataFrame.from_dict(columns)\n",
    "\n",
    "\n",
    "print(scraped_flip)\n",
    "\n",
    "\n",
    "#scraped_flip.to_csv(\"scraped_flip2.csv\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f087b-7f5d-4b09-8bcd-6c75b319f745",
   "metadata": {},
   "source": [
    "# flipkart each product in multipage"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8bde297-0ccf-4d93-acf2-b3da18b808b8",
   "metadata": {},
   "source": [
    "<span class=\"VU-ZEz\">Motorola G34 5G (Ocean Green, 128 GB)&nbsp;&nbsp;(8 GB RAM)</span>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c63450e-43d5-4aea-985c-22b0f62b31a9",
   "metadata": {},
   "source": [
    "<div class=\"Nx9bqj CxhGGd\">₹11,999</div>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09df94a5-803c-42e7-8c3f-4ac9b66b364f",
   "metadata": {},
   "source": [
    "<div class=\"XQDdHH\">4.2<img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMyIgaGVpZ2h0PSIxMiI+PHBhdGggZmlsbD0iI0ZGRiIgZD0iTTYuNSA5LjQzOWwtMy42NzQgMi4yMy45NC00LjI2LTMuMjEtMi44ODMgNC4yNTQtLjQwNEw2LjUuMTEybDEuNjkgNC4wMSA0LjI1NC40MDQtMy4yMSAyLjg4Mi45NCA0LjI2eiIvPjwvc3ZnPg==\" class=\"Rza2QY\"></div>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee8025bf-df3d-4239-9560-b8d92ff7c4fc",
   "metadata": {},
   "source": [
    "<span class=\"Wphh3N\"><span><span>486 Ratings&nbsp;</span><span class=\"hG7V+4\">&amp;</span><span>&nbsp;27 Reviews</span></span></span>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "586a6597-7a7f-4727-aca3-044b86792820",
   "metadata": {},
   "source": [
    "<div class=\"XQDdHH\">4.2<img src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMyIgaGVpZ2h0PSIxMiI+PHBhdGggZmlsbD0iI0ZGRiIgZD0iTTYuNSA5LjQzOWwtMy42NzQgMi4yMy45NC00LjI2LTMuMjEtMi44ODMgNC4yNTQtLjQwNEw2LjUuMTEybDEuNjkgNC4wMSA0LjI1NC40MDQtMy4yMSAyLjg4Mi45NCA0LjI2eiIvPjwvc3ZnPg==\" class=\"Rza2QY\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77854998-a961-4cae-9e78-18b8f7df7a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0849bb71-2d0c-45c2-8af3-d703848b815b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'URL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21748\\3888838951.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# Pull the webpage to variable called webpage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mwebpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mHEADERS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwebpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'URL' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_product_name(soup):\n",
    "    try:\n",
    "        product_name = soup.find(\"span\", attrs={\"class\": \"VU-ZEz\"})\n",
    "        product_name_text = product_name.text\n",
    "        product_name_string = product_name_text.strip()\n",
    "    except AttributeError:\n",
    "        product_name_string = \"data not avilable\"\n",
    "    return product_name_string\n",
    "\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price = soup.find(\"div\", attrs={\"class\": \"Nx9bqj CxhGGd\"}).string.strip()\n",
    "        # price_text = price.text\n",
    "        # price_text_string = price.strip()\n",
    "        \n",
    "        \n",
    "        \n",
    "    except AttributeError:\n",
    "        price = \"attribute error\"\n",
    "    return price\n",
    "\n",
    "def get_rating(soup):\n",
    "    try:\n",
    "        rating = soup.find(\"div\", attrs={'class': 'XQDdHH'}).string.strip()\n",
    "    except AttributeError:\n",
    "        rating = \"n/a\"\n",
    "    return rating\n",
    "\n",
    "def get_reviews(soup):\n",
    "    try:\n",
    "        review = soup.find(\"span\", attrs={'class': 'Wphh3N'}).string.strip()\n",
    "    except AttributeError:\n",
    "        review = \"n/a\"\n",
    "    return review\n",
    "\n",
    "BASE_URL = \"https://www.flipkart.com/search?q=mobile+5g\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    'Accept-Language': 'en-US, en;q=0.5'\n",
    "}\n",
    "\n",
    "# Pull the webpage to variable called webpage\n",
    "webpage = requests.get(URL, headers=HEADERS)  \n",
    "soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "print(webpage)\n",
    "\n",
    "# Separate links and store links\n",
    "links = soup.find_all(\"a\", attrs={'class': 'CGtC98'})\n",
    "product_list = []\n",
    "\n",
    "print(links)\n",
    "# Iterate through the links list and convert href to actual http link\n",
    "for link in links:\n",
    "    product_list.append(\"https://www.flipkart.com\" + link.get(\"href\"))\n",
    "    \n",
    "\n",
    "\n",
    "# Create a dictionary to store all scraping values\n",
    "columns = {\"Product_name\": [], \"price\": [], \"rating\": [], \"user_reviews\": []}\n",
    "\n",
    "# Iterate through links and get values of each webpage then store the values into dictionary.\n",
    "for link in product_list:\n",
    "    new_webpage = requests.get(link, headers=HEADERS)\n",
    "    new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "    # print(new_soup)\n",
    "    \n",
    "    columns[\"Product_name\"].append(get_product_name(new_soup))\n",
    "    columns[\"price\"].append(get_price(new_soup))\n",
    "    columns[\"rating\"].append(get_rating(new_soup))\n",
    "    columns[\"user_reviews\"].append(get_reviews(new_soup))\n",
    "\n",
    "# Create a data frame and store the dictionary values, then convert the data frame into a csv file.\n",
    "flip_df = pd.DataFrame.from_dict(columns)\n",
    "\n",
    "\n",
    "print(flip_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed6718-2bcd-47de-b792-9bba371ff36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "042a9c93-38cf-4ee8-bf51-43cf917570e5",
   "metadata": {},
   "source": [
    "# Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a21c1-35c1-4aaf-9b68-0a189a686573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved the page 1\n",
      "Successfully retrieved the page 2\n",
      "Successfully retrieved the page 3\n",
      "Successfully retrieved the page 4\n",
      "Successfully retrieved the page 5\n",
      "Successfully retrieved the page 6\n",
      "Successfully retrieved the page 7\n",
      "Successfully retrieved the page 8\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "BASE_URL = \"https://www.amazon.in/s?k=motorolla&rh=n%3A1389401031&ref=nb_sb_noss\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    'Accept-Language': 'en-US, en;q=0.5'\n",
    "}\n",
    "\n",
    "product_names = []\n",
    "ratings = []\n",
    "prices = []\n",
    "\n",
    "\n",
    "NUM_PAGES = 10\n",
    "\n",
    "\n",
    "for page in range(1, NUM_PAGES + 1):\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    URL = BASE_URL + str(page)\n",
    "    response = requests.get(URL, headers=HEADERS)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Successfully retrieved the page {page}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page {page}, status code: {response.status_code}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    for product in soup.find_all(\"div\", class_=\"KzDlHZ\"):\n",
    "        product_names.append(product.get_text())\n",
    "        \n",
    "\n",
    "    for rating in soup.find_all(\"div\", class_=\"XQDdHH\"):\n",
    "        ratings.append(rating.get_text())\n",
    "        \n",
    "    for price in soup.find_all(\"div\", class_=\"Nx9bqj _4b5DiR\"):\n",
    "        prices.append(price.get_text())\n",
    "        \n",
    "\n",
    "\n",
    "def set_100(max_num):\n",
    "    if len(product_names)>max_num:\n",
    "        product_names[max_num:]=[]\n",
    "    if len(ratings)>max_num:\n",
    "        ratings[max_num:]=[]\n",
    "    if len(prices)>max_num:\n",
    "        prices[max_num:]=[]\n",
    "    \n",
    "set_100(200)\n",
    "\n",
    "\n",
    "columns = {\"Product_Name\": product_names, \"Rating\": ratings, \"Price\": prices}\n",
    "\n",
    "print(len(product_names))\n",
    "print(len(ratings))\n",
    "print(len(prices))\n",
    "\n",
    "scraped_flip = pd.DataFrame.from_dict(columns)\n",
    "\n",
    "\n",
    "print(scraped_flip)\n",
    "\n",
    "\n",
    "scraped_flip.to_csv(\"scraped_flip2.csv\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f002c1-1064-47f1-bfb1-b64905fc9db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
